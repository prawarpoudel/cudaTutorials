In one of the earlier post for matrix multiplication, we saw that we can launch threads such that each thread will operate on a row of final output matrix to compute the resultant matrix. In next example, we saw that we can create a grid with dimension the same as the final output matrix and assign each thread in the grid to compute one element in the final output matrix.

We will use the same concept of assigning each thread with task of computing a single element of the final output matrix but we will try to be more clever. We can see from previous example that although multiple threads computed values for final matrix, the data accessed by the threads we consumed by them only. This means that there was no coordination between the threads in terms of values they accessed from the global scope.

It so happens in the CUDA architecture that the values accessed by a thread can be shared with the threads in the same thread block. This is provided by a special memory type called shared memory. But, first we need to figure out what will be the appropriate values to save in shared memory. It turns out the memory values that a thread needs for the calculation of its final output as at some point needed by other threads. So we will take a strategy such that the value needed to compute output of each thread will be stored in shared memory, then we will pass or share the partial result computed in the blocks to other blocks.

Since both our examples work in arbitrary sized rectangular matrices we will try to implement the same here contrary to most of the examples found in textbooks where the explanation is resorted to square matrix for most part.

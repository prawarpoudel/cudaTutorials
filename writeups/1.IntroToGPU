Computers are getting increasingly parallel. Multicore era has reached its peak with computers having many number of cores (as much as 64 cores in Intel® Xeon Phi™ Processor 7210 ). This allows so much power to the application end that an user can have 64 independent tasks that are not time scheduled (or a task 64 times as faster if the program is written appropriately). 
These cores should act as a complete CPU unit. However, in certain tasks that are more computation oriented, there is no need for a  core to have the power of a complete CPU. These cores can just be instructed by some controller sitting on top of them, and treat these so-called cores merely as a computational unit. 
This allows the computational units to be light weight in terms of memory requirements. A processor core customarily has cache, multiple levels of them, separate contorl unit and many more components. But for a computational unit, we do not need separate control unit and memory can be shared among many of them. Infact, we can group them together to share sort-of cache memory which is small in size and have another larger memory structure that is shared and accessible to all of them. 
The structure defined above allows more such computational units to be packed into a single die, thousands and probably millions of them. This is the idea of massively parallel hardwares, which gave rise to massively parallel programming paradigm.
GPUs were designed originally to assist in graphics applications. More on this can be found online everywhere, but as a refresher, I plan to write on it in future. But the idea is GPUs already had multiple processing units so it was only a matter of time someone started using it for other parallel computing tasks. NVIDIA took the matter to hand in 2007 and started creating the general-purpose computing paradigm for GPUs which is famous by the name CUDA. Before the introduction of CUDA, a programmer had to write his/her code using OpenGL or Direct3D technique to leverage power of GPU. Today, GPUs have more programming paradigm and is a lot easier to program massively-parallel processor than it was 10 years ago.
When writing a program that uses GPU for computation, the program has some sections that run on the CPU and some sections that run on GPU. This is the idea behind Heterogenous Computing. The CPU is called Host while the GPU is called the guest. For applications that are strictly computation related, the CPU has to offload data to the device for the device to computer. Device operates on the data using numerous threads it has and sends the result back to the CPU. Although there are recent advancements in using unified memory between host and device. In a unified memory, both the CPU and GPU can access the memory location without the CPU explicitly moving the data to and from GPU. Please visit https://devblogs.nvidia.com/unified-memory-cuda-beginners/ for more information on this topic although I might do some example and present a writeup at some point in future.
**SM and SP** Architecturally, a GPU consists of a number of multiprocessors with multiple processors inside them. The multiprocessor units are called Streaming Multiprocessors (SMs) which has a number of processors called Streaming Processors (SPs). A multiprocessor or SM will have memory that is shared between the SPs in that SM. SPs in a SM also share control logic.
**Grid, Block and Thread** When writing a program in CUDA, the program launches threads that are organized in Grid. This Grid can be viewed as a 3D structure of Blocks. This means Grid contains Blocks in x-, y- and z- directions. Such blocks in a grid are identified by the blockIdx.x, blockIdx.y and blockIdx.z.
Each of  these Blocks are again made of Threads. Blocks are themselves 3D structure which means there are Threads in x-, y- and z- directions. The Threads in each of the block are identified as threadIdx.x, threadIdx.y and threadIdx.z. Thread is the lowest level of hierarchy but is available with resources like local memory and registers.
**Warps**